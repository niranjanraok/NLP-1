\documentclass[11pt]{article}
\usepackage{cs65f10}
\usepackage{times}
\usepackage{latexsym}
\usepackage{verbatim}
\usepackage{graphicx}
\setlength\titlebox{6.5cm}    % Expanding the titlebox

\title{Finding Our Roots: A Hybrid Approach to Word Segmentation}

\author{Nick Schultz\\
  Swarthmore College '11\\
  {\tt nschult1@swarthmore.edu}  
  \And                            %omit if
  Kevin Pytlar\\                 %there isn't
  Swarthmore College '12\\
  {\tt kpytlar1@swarthmore.edu}}

\date{}

\begin{document}
\maketitle
\begin{abstract}
Product performance in the market is a topic heavily researched by analysts and
anyone else interested in the performance of companies and/or the economy.
Product sentiment is a main influencing factor in the performance of the
product in the market place.  The following paper will describe techniques used
for analyzing the sentiment of Twitter users toward different products.
\end{abstract}

\section{Introduction}
Sentiment analysis can be heavily quantitative, which we will cover in more
detail  in the methods section of the paper.  Quantitative trading techniques
are gaining popularity amongst banks and traders.  However, these quantitative
techniques can be based purely on the movement of the market, which raises
question as to how much the buying and selling reflects actual market demand.
Twitter can be used to collect data which directly reflects the market because
actual people are tweeting their opinions about market products.  Thus,
successfully analyzing the sentiment of tweets can lead to actual quantitative
demand and supply information that truly reflects the sentiment of the market.
(market?)Economic analysis using tweets as a data set can result in more
realistic trading practices because it bases the practices on actual demand
statistics.  This will help the market reflect a rational market (rather than
what?) as it is supposed to.

In a broader sense, sentiment analysis methods can be applied to any data set
and can gather different sets of quantitative information which can be used to
assess and analyze very different topics.  A more efficient sentiment analyzer
can help push the effectiveness of natural language processing techniques, and
further help in the effort to understand language through the use of computer
algorithms.  When interacting with other people, it is obviously important to
understand the opinion or sentiment of what is being discussed .  If we can
teach computers to understand the sentiment or opinion of people, we can start
to understand the overall sentiment of large sets of communication data,
because of the capabilities of computers to process large amounts of data so
quickly.  Understanding the sentiment of a crowd can help to improve the
quality of service in a vast amount of fields.

We used Twitter to collect data on smart phones, specifically the iPhone 4, Evo
4G and Windows Phone.  Twitter¿s API is free, so we were easily able to connect
to their servers and search for the products we selected.  We ran searches for
1 month, compiling millions of tweets from many different users about the same
topics. Each tweet could contain any one or all of the search topics.  Using
these tweets, we can use different sentiment analysis techniques to quantify
the sentiment towards the product.

Our sentiment analysis was semi-supervised.  Completely supervised methods
consist of hand-making or using an already made list of sentiment words and
sentiment scores.  Finding the sentiment of a tweet can be as simple as finding
words in tweets that are in the sentiment list and adding together the
sentiment scores of the words that are found in the tweet.  If the final score
is positive, positive sentiment is expected, if the score is negative, negative
sentiment is expected. The implementation section describes how our program is
semi-supervised and how it analyzes the data which it collects.  Additionally
to describing our program and the results, we will connect our project with
other sentiment analysis papers.  For example, how we could incorporate other
methods described in other papers to continue to our sentiment analyzer.  We
will also discuss how our methods build on the methods in other papers.  In our
evaluation section, the paper will discuss what sort of experiments we ran,
what data we got from those experiments and how that data shows the the
program's skill at classifying sentiment in tweets. 

\section{Methods}
The main goal of our project was to create an algorithm that learned the
vocabulary of the positive and negative words in the corpus it was trained on.
We did this in two steps, first aggregating a score for each word in the
corpus, then deciding which of those words to add to our improved sentiment
list.  

\subsection{Original Sentiment List}
First we built a small sentiment list of words with sentiment scores.  The
scores are positive or negative, and scored 1-3 based on how positive or
negative we thought the word is.  The list is made up of language that is
clearly positive and negative, there are no words that are neutral or could
possibly go either way.  We also tried to use words for this initial sentiment
list that would be used in a medium like Twitter, and that had clear meaning in
that context.  The list we created was  27 words long and had similar total
sentiment scores for positive and negative feeling.  

\subsection{Data to Analyze}
We collected millions of tweets, in English, which are about smart phones.  We
can do this by using the python Twitter API, specifically the search() method.
In the search method we were able to specify the language to English and the
subject of the tweets we want.  As mentioned in the introduction, we were
searching for tweets relating to the iPhone 4, Evo 4G and Windows Phone.
Because Twitter is such an open forum, there are a lot of spam messages,
especially messages that pertain to consumer electronics.  As such, we worked
to eliminate duplicate messages, because they were most likely to be spam.  We
also cut out usertags (i.e. @twitteruser) and urls from the tweets, as those
would have no sentiment attached to them.  After all of our parsing we cut the
10 million tweets we mined from Twitter down to around 250,000 unique tweets.  

\subsection{Score Aggregation}
To get new scores for words in our tweet corpus, we look for currently scored
words that occur in the same tweet.  We sum up the total sentiment of words in
a tweet, using sentiment provided by the sentiment list.  If a given word is
not in the sentiment list, it contributes 0 to the sum.  Once that sum is
computed, each word adds it to the words running total of scores of tweets it
is in.  We also keep track of how many times the word shows up and how many
tweets it appears in.  

\subsection{Sentiment Weight Algorithm/Adding to sentiment list}
Once we have the set of words and the scores associated with them, we run the
part of our algorithm that improves our sentiment word list.  We have two
thresholds which we use to determine whether a given word should be added to
our improved sentiment list.  If a word is selected to be added to the
sentiment list, the value it uses is the average score per instance of the
word.  The first threshold we look at is the percentage of tweets the word is
used in.  We want to have a lower bound on this percentage because if words
which were just in one tweet were allowed into the sentiment analyzer, the
score it would be added with would not be averaged over a large number of
appearances, so the score would skew the sentiment list.

The second threshold eliminates words that are not strongly correlated with one
sentiment or the other.  We look at the list of words and scores that got
through the percentage of tweets threshold and take a given percentage of them
off the top.  The idea behind this is to only add the most polarizing words,
and eliminate the words that can be used in either sense.  Words that were in
the previous sentiment list are not given any sort of advantage over new words
to be added, besides the fact that the words already in the sentiment list are
guaranteed to have at least their sentiment score in the next round.

\subsection{Scoring Tweets}
Once we have a comprehensive sentiment list, created using several iterations
of the bootstrapping algorithm described in the two previous sections, we are
ready to score some tweets.  To score tweets we count up sentiment based on the
final sentiment list we have.  The scorer looks a lot like the score
aggregation algorithm, with a few added features.  We can score tweets by
looking at each word in a tweet; if the word is also in our sentiment list we
can add the sentiment score of that word to a total sentiment score for the
entire tweet. We added a negation scoring system.  If we find a word in a tweet
that is also in our sentiment list, before adding the sentiment score of the
word to the score of the tweet, we look to see if the word 'not' is one or two
words before the current word in the sentence.  If 'not' is there, we switch
the polarity of the sentiment score of the current word then add it to the
score of the tweet.

We have also implemented a word enhancer scoring method.  If we use this in
scoring a tweet, we look for enhancing words such as 'so' or 'incredibly' one
place before the current word in the tweet which was found in our sentiment
list.  If we find an enhancing word, we multiply the sentiment score of the
current word by a weight and then add the weighted sentiment score of the
current word to the sentiment of the entire tweet.  Like our original sentiment
list, this enhancer list was created by hand. If we find a negation then an
enhancer, for example, 'not so great', we do not use the enhancer.  This is
because instead of enhancing the emphasis on great, ¿not so¿ decreases the
sentiment score of great.  To avoid this, we do not look for enhancing words
where negation is also found.

\section{Related Work}

\subsection{\cite{Yahoo!}}
Yahoo! for Amazon:Sentiment Extraction from Small Talk on the Web attempts to
use sentiment analysis algorithms to categorize the sentiment of analyst's post
on stock message boards and they use this sentiment data to see if they can
find relationships between the sentiment data and stock values.  Their goal is
very similar to ours: to use sentiment analysis on a data set to predict
company/stock performance, however our methods vary.  Das and Chen use a Naive
Classifier, Vector Distance Classifier, Discriminant-Based Classifier,
Adjective-Adverd Phrase Classifier, Bayesian Classifier, and Voting amongst
Classifiers.  To examine the results of their classifiers the utilize four
metrics: percentage classification accuracy, percentage of false positives,
percentage error in aggregate sentiment, and a chi squared test of no
classification ability.  Their methods ended up not predicting individual stock
prices well, and they stretched to conclude they could predict index values.
We expect the differences in data sets used between our experiments and their's
could enhance the predictive abilities of our methods compared to the
predictions they were capable of.

\subsection{\cite{Written_Expression}}
Dodds and Danforth use sentiment analysis on songs, blogs, and Presidential
speeches.  They then draw conclusions from the sentiment data they have
gathered to relate sentiment to things such as time period, location of person,
and gender.  Again, a large difference is the data set we used compared to the
data set they used.  However, we drew inspiration from the analysis they did on
their methods and the conclusions they were capable of reaching from their
sentiment analysis. 

\subsection{\cite{Mixture}}
The model described in Topic Sentiment Mixture: Modeling Facets and Opinions in
Weblogs attempts to model the sentiment of weblogs.  A unique feature of their
sentiment analyzer is they extract subtopics from the particular blog entry.
Thus, if the blog entry is about the iPhone, their sentiment anlayzer may
conclude positive sentiment for the iPhone but negative sentiment for the cost
of the iPhone.  In this case, cost would be the subtopic.  This is somewhat
related to what we conclude using our final list of sentiment words.  For
example, if we see case has negative sentiment after running our sentiment
anlayzer on tweets which the topic is iPhone, we know people have negative
sentiment towards the cases of iPhones.  This is similar to the subtopic system
used in their methods, but the implementation between our method and their
method varies.  We also drew inspiration from the predictive capabilities of
sentiment anlaysis that the authors speak of in their 'INTRODUCTION' section.

\subsection{\cite{Tweets_to_Polls}}
From Tweets to Polls, uses tweets from Twitter as a data set in an effort to
score each tweet¿s sentiment.  They also start with a list of sentiment words
which each have a positive or negative sentiment score.  To score each tweet
they find sentiment words in the tweet then add up the sentiment score of each
sentiment word found in that tweet.  If the resulting score is positive, the
tweet is concluded to have positive sentiment, if the score is negative, the
tweet has negative sentiment.  This is a fairly simple algorithm, considering
it is fully supervised.  We used this paper as inspiration to use tweets as a
data set.  It was also a good paper to start with since the implementation of
sentiment analysis was very straight-forward.  We could easily build on their
methods by using things such as negation, enhancers, and making our model
semi-supervised.  


\section{Evaluation}
To evaluate our system, we created a gold standard.  To create the standard we
hand-scored all of the words in 50 tweets with either a positive, negative or
0.  Because of the normalization of the sentiment values in our sentiment list
improvement, most of our sentiment values were around 1, so the difference in
magnitude was not a factor.  After we hand scored our gold standard, we trained
a sentiment list on all of the other tweets for each variation of our algorithm
based on the thresholds and number of times run through the sentiment list
improvement algorithm.  The values we tested on can be seen in table
~\ref{fig:exParams}. 



\begin{figure}
  \caption{Experiment Parameters}
  \begin{tabular}{ | l | c | c | c |} \hline 
    Factor & Low & High & Step \\ \hline\hline
    Top Percent & 10 & 30 & 10 \\ \hline
    Percentage of Tweets & .01 & .1 & .02\\ \hline
    Number of Iterations & 1 & 5 & 1 \\ \hline
  \end{tabular}
  \label{fig:exParams}
\end{figure}

\section{Analysis}

\section{Future Work}

\section{Conclusions}
The data set we used has potential to be quite predictive, while the analyst
message on stock message boards data set used by Das and Chen and others was
proven to not have much predictive value: "Antweiler and Frank (2004) examine
the bullishness of messages and find that while web talk does not predict stock
movements, it is predictive of volatility.  \cite{Yahoo!}"  This makes sense; the message
boards could tell them how volatile the intraday trading would be, as analysts
with differing opinion try to out muscle each other creating volatile markets,
but not whether the stock would trend higher or lower, which is dependent on
actual market revelations as opposed to analyst opinion.  How do our methods
create possible predictive power?  First, we can look at tweet volume.  Then
based on sentiment statistics of those tweets, we expect we can predict future
volume changes based on current sentiment.  For example, if in one month iPhone
has more tweet volume than blackberry, but worse overall tweet sentiment than
blackberry, we would expect iPhone tweet volume to decrease at some point as
more people bought blackberry's due to more satisfaction with that product.
Obviously, this type of predictive capability could only be achieved after
substantial analysis of sentiment data which contain evenly spread sentiment
statistics of tweets over a period of time.  Comparing these statistics with
actual product performance could let us use our data gathered to start
predicting product performance.  Topic Sentiment Mixture: Modeling Facets and
Opinion in Weblogs mentions the predictive power of sentiment analysis:
"Although much work has been done recently on blog mining, most existing work
aims at extracting and analyzing topical contents of bog articles without any
analysis of sentiments in an article.  The lack of sentiment analysis in such
work often limits the effectiveness of the mining results.  For example, [...]
a burst of blog mentions about a book has been shown to be correlated with a
spike of sales of the book in Amazon.com.  However, a burst of criticism of a
book is unlikely to indicate a growth of the book sales.  Similarly a decrease
of blog mentions about a product might actuallyt be caused by the decrease of
complaints about its defects.  \cite{Mixture}"
\bibliographystyle{cs65f10}
\bibliography{cs65f10}

\end{document}
